{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, GlobalAveragePooling2D \n",
    "from keras.layers.normalization import batch_normalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import albumentations as albu\n",
    "import efficientnet.keras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        myDict = pickle.load(fo, encoding='latin1')\n",
    "    return myDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../Datasets/cifar-100-python/'\n",
    "trainData = unpickle(PATH +'train')\n",
    "#type of items in each file\n",
    "for item in trainData:\n",
    "    print(item, type(trainData[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainData['data']))\n",
    "print(len(trainData['data'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(trainData['fine_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(trainData['coarse_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainData['batch_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = unpickle(PATH + 'test')\n",
    "metaData = unpickle(PATH + 'meta')\n",
    "#metaData\n",
    "print(\"Fine labels:\", metaData['fine_label_names'], \"\\n\")\n",
    "print(\"Coarse labels:\", metaData['coarse_label_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing coarse labels along with its number code in a dataframe\n",
    "category = pd.DataFrame(metaData['coarse_label_names'], columns=['SuperClass'])\n",
    "#storing fine labels along with its number code in a dataframe\n",
    "subCategory = pd.DataFrame(metaData['fine_label_names'], columns=['SubClass'])\n",
    "print(category)\n",
    "print(subCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainData['data']\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4D array input for building the CNN model using Keras\n",
    "X_train = X_train.reshape(len(X_train),3,32,32).transpose(0,2,3,1)\n",
    "#X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating a random number to display a random image from the dataset along with the label's number and name\n",
    "#setting the figure size\n",
    "rcParams['figure.figsize'] = 2,2\n",
    "#generating a random number\n",
    "imageId = np.random.randint(0, len(X_train))\n",
    "#showing the image at that id\n",
    "plt.imshow(X_train[imageId])\n",
    "#setting display off for the image\n",
    "plt.axis('off')\n",
    "#displaying the image id\n",
    "print(\"Image number selected : {}\".format(imageId))\n",
    "#displaying the shape of the image\n",
    "print(\"Shape of image : {}\".format(X_train[imageId].shape))\n",
    "#displaying the category number\n",
    "print(\"Image category number: {}\".format(trainData['coarse_labels'][imageId]))\n",
    "#displaying the category name\n",
    "print(\"Image category name: {}\".format(category.iloc[trainData['coarse_labels'][imageId]][0].capitalize()))\n",
    "#displaying the subcategory number\n",
    "print(\"Image subcategory number: {}\".format(trainData['fine_labels'][imageId]))\n",
    "#displaying the subcategory name\n",
    "print(\"Image subcategory name: {}\".format(subCategory.iloc[trainData['fine_labels'][imageId]][0].capitalize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16 random images to display at a time along with their true labels\n",
    "#setting the figure size\n",
    "rcParams['figure.figsize'] = 8,8\n",
    "#number of columns and rows in which images needs to be displayed\n",
    "num_row = 4\n",
    "num_col = 4\n",
    "#to get 4 * 4 = 16 images together\n",
    "imageId = np.random.randint(0, len(X_train), num_row * num_col)\n",
    "#creating subplots\n",
    "fig, axes = plt.subplots(num_row, num_col)\n",
    "#main title of the plot\n",
    "plt.suptitle('Images with True Labels', fontsize=18)\n",
    "#displaying images as subplots\n",
    "for i in range(0, num_row):\n",
    "    for j in range(0, num_col):\n",
    "        k = (i*num_col)+j\n",
    "        axes[i,j].imshow(X_train[imageId[k]])\n",
    "        axes[i,j].set_title(subCategory.iloc[trainData['fine_labels'][imageId[k]]][0].capitalize())\n",
    "        axes[i,j].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the testing dataset\n",
    "X_test = testData['data']\n",
    "X_test = X_test.reshape(len(X_test),3,32,32).transpose(0,2,3,1)\n",
    "y_train = trainData['fine_labels']\n",
    "y_test = testData['fine_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of classes in the dataset\n",
    "n_classes = 100\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stratified shuffle split to split training set into training and validation set to preserve the percentage of samples in each of the 100 classes\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for train_index, val_index in sss.split(X_train, y_train):\n",
    "    X_train_data, X_val_data = X_train[train_index], X_train[val_index]\n",
    "    y_train_data, y_val_data = y_train[train_index], y_train[val_index]\n",
    "\n",
    "print(\"Number of training samples: \", X_train_data.shape[0])\n",
    "print(\"Number of validation samples: \", X_val_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 224\n",
    "width = 224\n",
    "channels = 3\n",
    "input_shape = (height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img, shape):\n",
    "    return cv2.resize(img, (shape[1], shape[0]), interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 100\n",
    "epochs = 15\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, images, labels=None, mode='fit', batch_size=batch_size, dim=(height, width), channels=channels, n_classes=n_classes, shuffle=True, augment=False):\n",
    "        \n",
    "        #initializing the configuration of the generator\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.channels = channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.on_epoch_end()\n",
    "   \n",
    "    #method to be called after every epoch\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(self.images.shape[0])\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    #return numbers of steps in an epoch using samples & batch size\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.images) / self.batch_size))\n",
    "    \n",
    "    #this method is called with the batch number as an argument to #obtain a given batch of data\n",
    "    def __getitem__(self, index):\n",
    "        #generate one batch of data\n",
    "        #generate indexes of batch\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index+1) * self.batch_size]\n",
    "        \n",
    "        #generate mini-batch of X\n",
    "        X = np.empty((self.batch_size, *self.dim, self.channels))\n",
    "        for i, ID in enumerate(batch_indexes):\n",
    "            #generate pre-processed image\n",
    "            img = self.images[ID]\n",
    "            #image rescaling\n",
    "            img = img.astype(np.float32)/255.\n",
    "            #resizing as per new dimensions\n",
    "            img = resize_img(img, self.dim)\n",
    "            X[i] = img\n",
    "            \n",
    "        #generate mini-batch of y\n",
    "        if self.mode == 'fit':\n",
    "            y = self.labels[batch_indexes]\n",
    "            \n",
    "            #augmentation on the training dataset\n",
    "            if self.augment == True:\n",
    "                X = self.__augment_batch(X)\n",
    "            return X, y\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            return X\n",
    "        \n",
    "        else:\n",
    "            raise AttributeError(\"The mode should be set to either 'fit' or 'predict'.\")\n",
    "            \n",
    "    #augmentation for one image\n",
    "    def __random_transform(self, img):\n",
    "        composition = albu.Compose([albu.HorizontalFlip(p=0.5),\n",
    "                                   albu.VerticalFlip(p=0.5),\n",
    "                                   albu.GridDistortion(p=0.2),\n",
    "                                   albu.ElasticTransform(p=0.2)])\n",
    "        return composition(image=img)['image']\n",
    "    \n",
    "    #augmentation for batch of images\n",
    "    def __augment_batch(self, img_batch):\n",
    "        for i in range(img_batch.shape[0]):\n",
    "            img_batch[i] = self.__random_transform(img_batch[i])\n",
    "        return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = DataGenerator(X_train_data, y_train_data, augment=True) \n",
    "valid_data_generator = DataGenerator(X_val_data, y_val_data, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = DataGenerator(X_train_data, y_train_data, augment=True) \n",
    "valid_data_generator = DataGenerator(X_val_data, y_val_data, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efnb0 = efn.EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape, classes=n_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(efnb0)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.0001)\n",
    "\n",
    "#early stopping to monitor the validation loss and avoid overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "#reducing learning rate on plateau\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience= 5, factor= 0.5, min_lr= 1e-6, verbose=1)\n",
    "#model compiling\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.fit_generator(train_data_generator, validation_data = valid_data_generator, callbacks = [early_stop, rlrop],verbose = 1, epochs = epochs)\n",
    "\n",
    "#saving the trained model weights as data file in .h5 format\n",
    "model.save_weights(\"cifar_efficientnetb0_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "\n",
    "plt.suptitle('Loss and Accuracy Plots', fontsize=18)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_history.history['loss'], label='Training Loss')\n",
    "plt.plot(model_history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of epochs', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of epochs', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss, valid_accuracy = model.evaluate_generator(generator = valid_data_generator, verbose = 1)\n",
    "\n",
    "print('Validation Accuracy: ', round((valid_accuracy * 100), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_generator(DataGenerator(X_test, mode='predict', augment=False, shuffle=False), verbose=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "test_accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "print('Test Accuracy: ', round((test_accuracy * 100), 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "print(cm)\n",
    "target = [\"Category {}\".format(i) for i in range(n_classes)]\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(y_pred)\n",
    "rcParams['figure.figsize'] = 12,15\n",
    "\n",
    "num_row = 4\n",
    "num_col = 4\n",
    "\n",
    "imageId = np.random.randint(0, len(X_test), num_row * num_col)\n",
    "\n",
    "fig, axes = plt.subplots(num_row, num_col)\n",
    "\n",
    "for i in range(0, num_row):\n",
    "    for j in range(0, num_col):\n",
    "        k = (i*num_col)+j\n",
    "        axes[i,j].imshow(X_test[imageId[k]])\n",
    "        axes[i,j].set_title(\"True: \" + str(subCategory.iloc[testData['fine_labels'][imageId[k]]][0]).capitalize() + \"\\nPredicted: \" + str(subCategory.iloc[prediction.iloc[imageId[k]]]).split()[2].capitalize(), fontsize=14)\n",
    "        axes[i,j].axis('off')\n",
    "        fig.suptitle(\"Images with True and Predicted Labels\", fontsize=18) \n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
